---
title: "NLP analysis of research articles identified as suspicious"
output: html_notebook
---


```{r}
# library(reticulate)
# reticulate::use_python("/usr/bin/python3", required = T)
# library(tidyverse)
# 
# # create a new environment 
# use_virtualenv("r-reticulate")
# 
# # install SciPy
# virtualenv_install("r-reticulate", "scipy")
# 
# # import SciPy (it will be automatically discovered in "r-reticulate")
# scipy <- import("scipy")
```


```{r}
library(pdftools)
library(tidyverse)
library(tidytext)

file_list <- c(
  "~/Downloads/4777-4783-MiR-214-blocks-osteoblast-differentiation-by-targeting-β-catenin.pdf",
  "~/Downloads/4703-4710-Combined-treatment-of-lymphoma.pdf",
  "~/Downloads/1551-1558-TGFb1-protects-myocardium-after-IR-injury.pdf" ,
  "~/Downloads/937-944-MiR-155-regulates-p27Kip1-in-oral-cancer.pdf", 
  "~/Downloads/5008-5016-Drug-resistance-in-colon-cancer-1.pdf",
  "~/Downloads/3e45b2c3-b22c-4f27-9fa7-852bb6aa2c42.pdf",
  "~/Downloads/pone.0073004.pdf",
  "~/Downloads/or_41_1_377_PDF.pdf",
  "~/Downloads/xiao2018.pdf",
  "~/Downloads/8833-8840.pdf"
  # "~/Downloads/10.1.1.913.6093.pdf"
  )

# pdf_file <- file.path("~/Downloads/4777-4783-MiR-214-blocks-osteoblast-differentiation-by-targeting-β-catenin.pdf")

if (exists("tidy_words")) {
  remove(tidy_words)
}

for (i in 1:length(file_list)) {
  
  pdf_file <- file.path(file_list[i])

text <- pdf_text(pdf_file)

text_df <- as_tibble(text)

text_lines <- text_df %>%
  mutate(article = pdf_file, 
         page = row_number(),
         line_txt = strsplit(as.character(value), "\n")) %>% 
  unnest(line_txt) %>%
  select(-value) %>%
  mutate_if(is.character, trimws) %>%
  mutate_all(na_if,"") %>%
  drop_na(line_txt) %>%
  mutate(line = as.numeric(as.factor(line_txt)))
  
if (!exists("tidy_words")) {

  tidy_words <- text_lines %>%
    unnest_tokens(word, line_txt)
  
} else {

  tidy_words_temp <- text_lines %>%
    unnest_tokens(word, line_txt)
  
  tidy_words <- rbind(tidy_words, tidy_words_temp)
}



}
```

```{r}
# text_df <- as_tibble(text)
# 
# text_lines <- text_df %>%
#   mutate(article = pdf_file, 
#          page = row_number(),
#          line_txt = strsplit(as.character(value), "\n")) %>% 
#     unnest(line_txt) %>%
#   select(-value) %>%
#   mutate_if(is.character, trimws) %>%
#   mutate_all(na_if,"") %>%
#   drop_na(line_txt) %>%
#   mutate(line = as.numeric(as.factor(line_txt)))
# 
# 
# tidy_words <- text_lines %>%
#   unnest_tokens(word, line_txt)

```

```{r}
cleaned_words <- tidy_words %>%
  anti_join(get_stopwords()) 

cleaned_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 0.5*max(n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(., aes(y = word, x =n)) +
  geom_col() +
  theme_bw() 

# see most commonly used words
cleaned_words %>%
  group_by(article) %>%
  count(word, sort = TRUE) %>%
  filter(n > 0.5*max(n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(., aes(y = word, x =n)) +
  geom_col() +
  theme_bw() +
  facet_wrap(~article, ncol = 2, scales = "free_y") 
```

Sentiment analysis
```{r}
positive <- get_sentiments("bing") %>%
  filter(sentiment == "positive")

tidy_words %>%
  filter(article == pdf_file) %>%
  semi_join(positive) %>%
  count(word, sort = TRUE)

negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

tidy_words %>%
  filter(article == pdf_file) %>%
  semi_join(negative) %>%
  count(word, sort = TRUE)
```
sentiment analysis cont
```{r}
bing <- get_sentiments("bing")

article_sentiment <- tidy_words %>%
  inner_join(bing) %>%
  count(article, index = line %/% 5, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

ggplot(article_sentiment, aes(index, sentiment, fill = article)) +
  geom_bar(stat = "identity", show.legend = FALSE)  +
  facet_wrap(~article, ncol = 3, scales = "free_x") +
  theme_bw()
```

```{r}
bing_word_counts <- tidy_words %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)

bing_word_counts

bing_word_counts %>%
  filter(n > 5) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment") +
  theme_bw()
```

```{r}
library(wordcloud)

cleaned_words %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

library(reshape2)

tidy_words %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```



```{r}
# pngfile <- pdftools::pdf_convert("~/Downloads/4777-4783-MiR-214-blocks-osteoblast-differentiation-by-targeting-β-catenin.pdf", dpi = 600)
# 
# text2 <- tesseract::ocr(pngfile)
# cat(text2)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

